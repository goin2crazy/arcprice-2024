{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"class Config: \n    \"\"\"\n    In this competition the matrix size is from 2x2 to 30x30\n    and maximum 5-6 examples of solutions \n    \"\"\"\n    \n    \n    train = {'inputs': '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json',\n             'outputs': '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json'}\n    validation = {'inputs': '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json', \n                  'outputs': '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json'}\n    test = {'inputs': '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'}\n    \n    max_size = (30, 30)\n    max_channels = 10\n    num_epoch = 150\n    batch_size = 16\n    buffer_size = 8\n    early_stopping_patience = 80 \n    \ncfg = Config() ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:10.214092Z","iopub.execute_input":"2024-06-26T21:21:10.214513Z","iopub.status.idle":"2024-06-26T21:21:10.269386Z","shell.execute_reply.started":"2024-06-26T21:21:10.214481Z","shell.execute_reply":"2024-06-26T21:21:10.267602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport json \n\nfrom tensorflow.data import Dataset \nimport tensorflow as tf \n\nfrom tensorflow.keras import layers \nimport keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T21:21:10.272140Z","iopub.execute_input":"2024-06-26T21:21:10.272633Z","iopub.status.idle":"2024-06-26T21:21:27.899076Z","shell.execute_reply.started":"2024-06-26T21:21:10.272589Z","shell.execute_reply":"2024-06-26T21:21:27.897694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation ","metadata":{}},{"cell_type":"code","source":"import cv2 \n\ndef size_to_30(i, key = 'input', target_size = None, for_tf = True):\n    if type(i) == dict: \n        arr = np.array(i[key])\n    else: \n        arr = np.array(i)\n        \n    if target_size == None: \n        target_size = cfg.max_size\n    # Convert arr to cv2 image (assuming arr is in a format compatible with OpenCV)\n    img = arr.astype(np.uint8)\n    # Resize the image to 30x30 using cubic interpolation\n    img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_NEAREST)\n    # Convert the resized image back to a NumPy array\n    if for_tf: \n        return tf.convert_to_tensor(img_resized)\n    else:\n        return img_resized\n\n\ndef prepare_inputs(path, only_one = True): \n    input_data = {\n        'test_inputs': list(), \n        'train_inputs': list(), \n        'train_outputs': list()}  \n    \n    with open(path, mode='r') as file: \n        data = json.load(file)\n        file.close() \n    \n    for v in data.values(): \n        if only_one: \n            input_data['test_inputs'].append([size_to_30(i) for i in v['test'][:1]])\n            input_data['train_inputs'].append([size_to_30(i) for i in v['train'][:1]])\n            input_data['train_outputs'].append([size_to_30(i, 'output') for i in v['train'][:1]])\n        else: \n            test_inputs = [size_to_30(i) for i in v['test']]\n            input_data['test_inputs'].extend([[i] for i in test_inputs])\n            \n            num_test_inputs = len(test_inputs)\n            \n            train_inputs = [size_to_30(i) for i in v['train'][:1]]\n            train_outputs = [size_to_30(i, 'output') for i in v['train'][:1]]\n            for i in range(num_test_inputs): \n                input_data['train_inputs'].append(train_inputs)\n                input_data['train_outputs'].append(train_outputs)\n        \n    return input_data \n    \ndef prepare_outputs(path, only_one = True): \n    output_data = list()\n    \n    with open(path, mode='r') as file: \n        data = json.load(file)\n        file.close() \n        \n    for v in data.values(): \n        if only_one: \n            output_data.append([size_to_30(i) for i in v[:1]])\n        else: \n            output_data.extend([[size_to_30(i)] for i in v])\n            \n    return output_data\n\ndef prepare1(data_path_item, only_one = True):\n    inputs = None \n    outputs = None \n    \n    \n    if 'inputs' in data_path_item.keys(): \n        inputs = data_path_item['inputs']\n        \n        inputs = prepare_inputs(inputs, only_one)\n        \n    if 'outputs' in data_path_item.keys(): \n        outputs = data_path_item[\"outputs\"]\n        \n        outputs = prepare_outputs(outputs, only_one)\n        \n    return inputs, outputs \n\nX_train, y_train = prepare1(cfg.train, only_one = False)\nX_validation, y_validation = prepare1(cfg.validation, only_one = False )\n# test = prepare1(cfg.test)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:27.901680Z","iopub.execute_input":"2024-06-26T21:21:27.902553Z","iopub.status.idle":"2024-06-26T21:21:29.121996Z","shell.execute_reply.started":"2024-06-26T21:21:27.902503Z","shell.execute_reply":"2024-06-26T21:21:29.120844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To Dataset ","metadata":{}},{"cell_type":"code","source":"def add_channels(item): \n    \"\"\"\n    item is 30x30 matrix which is tensor with shape [1, 30, 30]\n    \n    This function returns this tensors with shape [1, 30, 30, 9]\n    \"\"\"\n    \n    num_classes = cfg.max_channels\n    \n    item_with_channels = tf.one_hot(item, num_classes)\n    \n    return item_with_channels \n\ndef build_dataset(X, y = None, batch = True, shuffle = True): \n    ds_input = (\n        Dataset.from_tensor_slices({**X})\n      .map(lambda i: {k: tf.squeeze(v) for k, v in i.items()})\n      .map(lambda i: {k: add_channels(v) for k, v in i.items()}))\n    \n    if y != None: \n        ds_output = (Dataset.from_tensor_slices(y)\n          .map(lambda i: tf.squeeze(i))\n          .map(lambda i: add_channels(i))\n                    )\n    \n        ds = Dataset.zip((ds_input, ds_output))\n    \n    else: \n        ds = ds_input\n    \n    if shuffle == True: \n        ds = ds.shuffle(cfg.buffer_size)\n    \n    if batch == True: \n        ds = ds.batch(cfg.batch_size)\n            \n    return ds.prefetch(cfg.buffer_size)\n    \nds_train = build_dataset(X_train, y_train)\nds_validation = build_dataset(X_validation, y_validation)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:29.123512Z","iopub.execute_input":"2024-06-26T21:21:29.123879Z","iopub.status.idle":"2024-06-26T21:21:30.002834Z","shell.execute_reply.started":"2024-06-26T21:21:29.123848Z","shell.execute_reply":"2024-06-26T21:21:30.001426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define the convolutional block with Batch Normalization, Layer Normalization, and specific initializers\ndef conv_block(item):\n    # Define the initializers for Conv2D\n    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n    bias_initializer = tf.keras.initializers.Zeros()\n    \n    # Apply Conv2D with specified initializers\n    item_conved = layers.Conv2D(\n        filters=64,  # Example filter size, adjust as needed\n        kernel_size=(3, 3),  # Example kernel size, adjust as needed\n        padding='same',\n        kernel_initializer=kernel_initializer,\n        bias_initializer=bias_initializer\n    )(item)\n    \n#     item_conved = layers.MaxPooling2D(pool_size=(2, 2))(item_conved)\n    \n    # Apply Batch Normalization with specific initializers\n    item_conved = layers.BatchNormalization(\n        beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2),\n        gamma_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n    )(item_conved)\n    \n    # Apply Layer Normalization with specific initializers\n    item_conved = layers.LayerNormalization(\n        beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2),\n        gamma_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n    )(item_conved)\n    \n    # Apply Dropout\n    item_conved = layers.Dropout(rate=0.2)(item_conved)\n    \n    return item_conved\n\ndef build_model(conv_cycling_train=2, conv_cycling_test=2):\n    max_channels = cfg.max_channels\n    # Take all input tensors\n    train_inputs = layers.Input(shape=[*cfg.max_size, max_channels], name='train_inputs')\n    train_outputs = layers.Input(shape=[*cfg.max_size, max_channels], name='train_outputs')\n    test_inputs = layers.Input(shape=[*cfg.max_size, max_channels], name='test_inputs')\n    \n    train_inputs_conved = None\n    train_outputs_conved = None \n    for i in range(conv_cycling_train):\n        train_inputs_conved = conv_block(train_inputs)\n        train_outputs_conved = conv_block(train_outputs)\n        \n    test_inputs_conved = None \n    for i in range(conv_cycling_test):\n        test_inputs_conved = conv_block(test_inputs)\n    \n    # Train data\n    train_ = layers.Add()([train_inputs_conved , train_outputs_conved ])\n    train_ = layers.Flatten()(train_)\n    train_ = layers.Dense(units=1024, activation='gelu')(train_)\n    \n    # Test data\n    test_ = layers.Flatten()(test_inputs_conved )\n    test_ = layers.Dense(units=1024, activation='gelu')(test_)\n    \n    # Merge train and test data\n    merged = layers.Add()([train_, test_])\n    merged = layers.Dense(units=1024, activation='gelu')(merged)\n    \n    # Final Dense layer to get 30x30 output\n    output = layers.Dense(units=cfg.max_size[0]*cfg.max_size[1]*cfg.max_channels, activation='softmax')(merged)\n    output = layers.Reshape([*cfg.max_size, max_channels])(output)\n    \n    model = tf.keras.models.Model(inputs=[train_inputs, train_outputs, test_inputs], outputs=output)\n    \n    return model\n\nmain_model = build_model()\nmain_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:30.006334Z","iopub.execute_input":"2024-06-26T21:21:30.006757Z","iopub.status.idle":"2024-06-26T21:21:31.418169Z","shell.execute_reply.started":"2024-06-26T21:21:30.006722Z","shell.execute_reply":"2024-06-26T21:21:31.416990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"class CustomLoss(tf.keras.losses.Loss):\n    def __init__(self, name=\"custom_categorical_loss\"):\n        super().__init__(name=name)\n        self.criterion = tf.keras.losses.CategoricalCrossentropy()\n\n    def call(self, y_true, y_pred):\n        # Ensure y_pred is in logits form if softmax is applied during the last layer of the model\n        y_true_flatten = layers.Flatten()(y_true)\n        y_pred_flatten = layers.Flatten()(y_pred)\n        \n        # Compute categorical crossentropy\n        loss = self.criterion(y_true, y_pred)\n        \n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:31.419690Z","iopub.execute_input":"2024-06-26T21:21:31.420139Z","iopub.status.idle":"2024-06-26T21:21:31.428777Z","shell.execute_reply.started":"2024-06-26T21:21:31.420088Z","shell.execute_reply":"2024-06-26T21:21:31.427134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_model.compile(optimizer='adam', \n              loss=CustomLoss(),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:31.430409Z","iopub.execute_input":"2024-06-26T21:21:31.430877Z","iopub.status.idle":"2024-06-26T21:21:31.457277Z","shell.execute_reply.started":"2024-06-26T21:21:31.430840Z","shell.execute_reply":"2024-06-26T21:21:31.455779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * 0.7\n\nscheduler = keras.callbacks.LearningRateScheduler(scheduler) \nearly_stopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', \n                                               min_delta = 1e-4, \n                                               patience = cfg.early_stopping_patience, \n                                               restore_best_weights = True)\n\nmain_model.fit(ds_train, \n               epochs = cfg.num_epoch, \n               validation_data = ds_validation, \n               callbacks = [\n                   scheduler, \n                   early_stopping,\n               ]\n              )","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:21:31.458387Z","iopub.execute_input":"2024-06-26T21:21:31.458746Z","iopub.status.idle":"2024-06-26T21:23:01.562445Z","shell.execute_reply.started":"2024-06-26T21:21:31.458715Z","shell.execute_reply":"2024-06-26T21:23:01.561091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission","metadata":{}},{"cell_type":"code","source":"X_test, _ = prepare1(cfg.test, only_one = False)\nds_test = build_dataset(X_test, shuffle = False )","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:23:01.564398Z","iopub.execute_input":"2024-06-26T21:23:01.564892Z","iopub.status.idle":"2024-06-26T21:23:02.011548Z","shell.execute_reply.started":"2024-06-26T21:23:01.564848Z","shell.execute_reply":"2024-06-26T21:23:02.009991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = list()\n\nfor batch in ds_test: \n    prediction = main_model(batch)\n    predictions.extend([i for i in prediction.numpy()])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:23:02.013735Z","iopub.execute_input":"2024-06-26T21:23:02.014310Z","iopub.status.idle":"2024-06-26T21:23:03.357027Z","shell.execute_reply.started":"2024-06-26T21:23:02.014245Z","shell.execute_reply":"2024-06-26T21:23:03.355699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_ids = list() \nactual_shapes = list()\n\nwith open(cfg.test['inputs'], mode='r') as file: \n    data = json.load(file)\n    file.close() \n\nfor k, v in data.items(): \n    test_input_num = len(v['test'])\n    \n    train_output_shape = [np.array(i['output']).shape for i in v['train'][:1]]\n    train_input_shape = [np.array(i['input']).shape for i in v['train'][:1]]\n    test_input_shape = [np.array(i['input']).shape for i in v['test'][:1]]\n    \n    target_shape = [(int(test_input_shape_[0] * (train_output_shape_[0] / train_input_shape_[0]))\n                     , int(test_input_shape_[1] * (train_output_shape_[1] / train_input_shape_[1])))\n                    for test_input_shape_, train_output_shape_, train_input_shape_ \n                        in zip(test_input_shape, train_output_shape, train_input_shape)]\n\n    \n    for i in range(test_input_num): \n        actual_shapes.extend(target_shape)\n        actual_ids.append(k)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:30:12.217542Z","iopub.execute_input":"2024-06-26T21:30:12.217993Z","iopub.status.idle":"2024-06-26T21:30:12.263149Z","shell.execute_reply.started":"2024-06-26T21:30:12.217956Z","shell.execute_reply":"2024-06-26T21:30:12.261884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_reshaped = [np.argmax(p, axis = -1) for p in predictions]\npredictions_reshaped = [cv2.resize(p.astype(np.uint8), t, interpolation=cv2.INTER_NEAREST) for p, t in zip(predictions_reshaped, actual_shapes)]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:31:53.361140Z","iopub.execute_input":"2024-06-26T21:31:53.361606Z","iopub.status.idle":"2024-06-26T21:31:53.373374Z","shell.execute_reply.started":"2024-06-26T21:31:53.361573Z","shell.execute_reply":"2024-06-26T21:31:53.371595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = dict() \nfor i, p in zip(actual_ids, predictions_reshaped): \n    if i in submission.keys(): \n        ...\n    else: \n        submission[i] = list() \n        submission[i].append(dict())\n        \n        submission[i][0][f'attempt_1'] = p.tolist() \n        submission[i][0][f'attempt_2'] = p.tolist()         ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:31:55.017044Z","iopub.execute_input":"2024-06-26T21:31:55.017609Z","iopub.status.idle":"2024-06-26T21:31:55.031147Z","shell.execute_reply.started":"2024-06-26T21:31:55.017556Z","shell.execute_reply":"2024-06-26T21:31:55.029556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.json', 'w') as file: \n    json.dump(obj = submission, fp = file)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:31:56.785489Z","iopub.execute_input":"2024-06-26T21:31:56.786000Z","iopub.status.idle":"2024-06-26T21:31:56.838603Z","shell.execute_reply.started":"2024-06-26T21:31:56.785965Z","shell.execute_reply":"2024-06-26T21:31:56.837217Z"},"trusted":true},"execution_count":null,"outputs":[]}]}